{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Musium Music playback daemon with a web-based library browser. Musium is an album-centered music player designed to run on an always-on device connected to speakers. Playback can be controlled from anywhere on the local network through the webinterface. Features Respects album artist and original release date metadata. Designed to scale to hundreds of thousands of tracks. User interface responds quickly, and indexing is fast. Optimized to run in resource-constrained environments, such as a Raspberry Pi. Responsive design, supports both mobile and desktop. Limitations Musium is not a tagger, it expects your files to be tagged correctly already. Musium is not a database, it does not store mutable data such as playcounts. It can log plays for processing by other tools, however. Supports only flac, with no intention to support other audio formats. Requires Linux, with no intention to become cross-platform. Uses raw Alsa, with no intention to support PulseAudio. Getting started Follow the building chapter to build from source. Then write a configuration file to musium.conf : listen = 0.0.0.0:8233 library_path = /home/user/music covers_path = /home/user/.cache/musium/covers audio_device = HDA Intel PCH Generate cover art thumbnails (requires Imagemagick and Guetzli): mkdir -p /home/user/.cache/musium/covers target/release/musium cache musium.conf Start the server: target/release/musium serve musium.conf You can now open the library browser at http://localhost:8233. Musium expects files to be tagged in a particular way, see the tagging chapter for more information.","title":"Overview"},{"location":"#musium","text":"Music playback daemon with a web-based library browser. Musium is an album-centered music player designed to run on an always-on device connected to speakers. Playback can be controlled from anywhere on the local network through the webinterface.","title":"Musium"},{"location":"#features","text":"Respects album artist and original release date metadata. Designed to scale to hundreds of thousands of tracks. User interface responds quickly, and indexing is fast. Optimized to run in resource-constrained environments, such as a Raspberry Pi. Responsive design, supports both mobile and desktop.","title":"Features"},{"location":"#limitations","text":"Musium is not a tagger, it expects your files to be tagged correctly already. Musium is not a database, it does not store mutable data such as playcounts. It can log plays for processing by other tools, however. Supports only flac, with no intention to support other audio formats. Requires Linux, with no intention to become cross-platform. Uses raw Alsa, with no intention to support PulseAudio.","title":"Limitations"},{"location":"#getting-started","text":"Follow the building chapter to build from source. Then write a configuration file to musium.conf : listen = 0.0.0.0:8233 library_path = /home/user/music covers_path = /home/user/.cache/musium/covers audio_device = HDA Intel PCH Generate cover art thumbnails (requires Imagemagick and Guetzli): mkdir -p /home/user/.cache/musium/covers target/release/musium cache musium.conf Start the server: target/release/musium serve musium.conf You can now open the library browser at http://localhost:8233. Musium expects files to be tagged in a particular way, see the tagging chapter for more information.","title":"Getting started"},{"location":"api/","text":"API Endpoints: GET /track/:track_id.flac : Return the track itself. GET /album/:album_id : Return json album metadata. GET /albums : Return a json list of all albums. GET /cover/:album_id : Return cover art in original resolution. GET /thumb/:album_id : Return downsampled cover art. GET /search?q= : Return json search results. GET /queue : Return the current play queue. PUT /queue/:track_id : Enqueue the track with the given id. GET /volume : Return the current volume. POST /volume/up : Increase the volume by 1 dB. POST /volume/down : Decrease the volume by 1 dB.","title":"API Reference"},{"location":"api/#api","text":"Endpoints: GET /track/:track_id.flac : Return the track itself. GET /album/:album_id : Return json album metadata. GET /albums : Return a json list of all albums. GET /cover/:album_id : Return cover art in original resolution. GET /thumb/:album_id : Return downsampled cover art. GET /search?q= : Return json search results. GET /queue : Return the current play queue. PUT /queue/:track_id : Enqueue the track with the given id. GET /volume : Return the current volume. POST /volume/up : Increase the volume by 1 dB. POST /volume/down : Decrease the volume by 1 dB.","title":"API"},{"location":"building/","text":"Building Musium is written in Rust and Purescript , so you need to have the build tools for these available. An easy way to get them is through the Nix package manager . The following command enters a shell in which all of the required build tools are available: nix run --command $SHELL This environment is also tested on CI . Nix is a convenience, not a requirement. You are free to source the build tools elsewhere, for example from your system package repositories. The webinterface is written in Purescript . There is a basic makefile that calls purs and psc-package : make -C app stat app/output/app.js The server will serve app.js and other static files alongside the API. The server itself is written in Rust and builds with Cargo: cargo build --release The binary can then be found in target/release/musium .","title":"Building"},{"location":"building/#building","text":"Musium is written in Rust and Purescript , so you need to have the build tools for these available. An easy way to get them is through the Nix package manager . The following command enters a shell in which all of the required build tools are available: nix run --command $SHELL This environment is also tested on CI . Nix is a convenience, not a requirement. You are free to source the build tools elsewhere, for example from your system package repositories. The webinterface is written in Purescript . There is a basic makefile that calls purs and psc-package : make -C app stat app/output/app.js The server will serve app.js and other static files alongside the API. The server itself is written in Rust and builds with Cargo: cargo build --release The binary can then be found in target/release/musium .","title":"Building"},{"location":"configuration/","text":"Configuration Musium reads all settings from a configuration file. The location of the config file is passed as an argument to the program. Config files consist of key-value pairs with = separator, and support # for comments. Example # Note: listening on port 80 requires CAP_NET_BIND_SERVICE. # If you want to run as an unprivileged user, use a port beyond 1024. listen = 0.0.0.0:80 library_path = /home/media/music covers_path = /var/cache/musium/covers audio_device = UMC404HD 192k audio_volume_control = UMC404HD 192k Output Settings The following settings are available. Unless noted otherwise, all options must be specified exactly once. listen The address and port to bind to, for example 0.0.0.0:80 . Use 0.0.0.0 as the address to listen for external connections and make the player available to the entire local network. Use localhost to listen only on loopback. The listen address is optional and defaults to 0.0.0.0:8233 . library_path The directory to recursively scan for flac files. covers_path The directory to store cover art thumbnails in. These are generated by the cache subcommand, and served by the serve subcommand. The directory must exist. play_log_path The file to write json-formatted playback events to. The format consists of one json object per line, where every object represents a single event. The play log path is optional: when not set, play logs are not written. audio_device The Alsa card used for playback. When the configured card cannot be found, Musium will list all of the cards that are available. Musium uses the Alsa hardware device directly, there is no need nor support for PulseAudio. audio_volume_control The Alsa simple mixer control that controls playback volume. Often there are controls named Master , PCM , and Speakers , but this differs from card to card. Use amixer scontrols to list available controls. Be sure to run this with the right privileges (possibly as superuser, or as a user in the audio group) to reveal all available controls. Musium assumes exclusive control over this mixer control, so you should not manipulate it manually with tools like Alsamixer after starting Musium. In particular, Musium adjusts the volume to perform loudness normalization, so even for a constant target playback volume, Musium will manipulate the mixer control.","title":"Configuration"},{"location":"configuration/#configuration","text":"Musium reads all settings from a configuration file. The location of the config file is passed as an argument to the program. Config files consist of key-value pairs with = separator, and support # for comments.","title":"Configuration"},{"location":"configuration/#example","text":"# Note: listening on port 80 requires CAP_NET_BIND_SERVICE. # If you want to run as an unprivileged user, use a port beyond 1024. listen = 0.0.0.0:80 library_path = /home/media/music covers_path = /var/cache/musium/covers audio_device = UMC404HD 192k audio_volume_control = UMC404HD 192k Output","title":"Example"},{"location":"configuration/#settings","text":"The following settings are available. Unless noted otherwise, all options must be specified exactly once.","title":"Settings"},{"location":"configuration/#listen","text":"The address and port to bind to, for example 0.0.0.0:80 . Use 0.0.0.0 as the address to listen for external connections and make the player available to the entire local network. Use localhost to listen only on loopback. The listen address is optional and defaults to 0.0.0.0:8233 .","title":"listen"},{"location":"configuration/#library_path","text":"The directory to recursively scan for flac files.","title":"library_path"},{"location":"configuration/#covers_path","text":"The directory to store cover art thumbnails in. These are generated by the cache subcommand, and served by the serve subcommand. The directory must exist.","title":"covers_path"},{"location":"configuration/#play_log_path","text":"The file to write json-formatted playback events to. The format consists of one json object per line, where every object represents a single event. The play log path is optional: when not set, play logs are not written.","title":"play_log_path"},{"location":"configuration/#audio_device","text":"The Alsa card used for playback. When the configured card cannot be found, Musium will list all of the cards that are available. Musium uses the Alsa hardware device directly, there is no need nor support for PulseAudio.","title":"audio_device"},{"location":"configuration/#audio_volume_control","text":"The Alsa simple mixer control that controls playback volume. Often there are controls named Master , PCM , and Speakers , but this differs from card to card. Use amixer scontrols to list available controls. Be sure to run this with the right privileges (possibly as superuser, or as a user in the audio group) to reveal all available controls. Musium assumes exclusive control over this mixer control, so you should not manipulate it manually with tools like Alsamixer after starting Musium. In particular, Musium adjusts the volume to perform loudness normalization, so even for a constant target playback volume, Musium will manipulate the mixer control.","title":"audio_volume_control"},{"location":"loudness/","text":"Loudness normalization Musium can normalize the perceptual playback loudness based on loudness information in flac tags. TODO: How is it handled, do we keep the softest track at 100% volume and make everything else softer? Or is there a configurable target loudness level? Tags Two tags affect loudness normalization: BS17704_TRACK_LOUDNESS BS17704_ALBUM_LOUDNESS The tags must store the integrated loudness as defined in ITU-R BS.1770-4 , for the track, and the concatenation of all tracks respectively. The value must be a decimal number, followed by the suffix \u201c LUFS \u201d for Loudness Units Full Scale. For example, -9.317 LUFS . The value for album loudness must be consistent across all tracks in the album. ReplayGain ReplayGain has been very influential for loudness normalization, and it is widely supported by both taggers and players, but unfortunately it has become ambiguous over time. ReplayGain does not store the loudness of the track directly, instead it stores the gain that is needed to bring the track to target loudness. The target loudness was initially well-defined, but tools started offering different options to accomodate to the reference loudness of different standards such as EBU R-128 , ATSC A/85 , and ReplayGain 2.0. This means that ReplayGain tags from different sources do not necessarily have the same meaning, which defeats the purpose of normalization. In practice this means that ReplayGain is only really useful if you can be certain that all tags were produced by the same program with the same settings. The BS17704_* tags used by Musium aim to sidestep this problem by storing the observed loudness, instead of the gain. The gain can easily be computed by the player, and the particular target loudness that is used is not important anyway for normalizing loudness in a collection of music. (It does matter when you want to match the loudness of your music to e.g. external streaming services.) Furthermore, by naming the tag after the revision of the standard, future revisions to BS.1770 will not create ambiguities in the meaning of existing tags. Writing BS.1770-4 tags The BS1770 flacgain utility can be used to analyze a collection of flac files, and to add BS17704_* tags.","title":"Loudness normalization"},{"location":"loudness/#loudness-normalization","text":"Musium can normalize the perceptual playback loudness based on loudness information in flac tags. TODO: How is it handled, do we keep the softest track at 100% volume and make everything else softer? Or is there a configurable target loudness level?","title":"Loudness normalization"},{"location":"loudness/#tags","text":"Two tags affect loudness normalization: BS17704_TRACK_LOUDNESS BS17704_ALBUM_LOUDNESS The tags must store the integrated loudness as defined in ITU-R BS.1770-4 , for the track, and the concatenation of all tracks respectively. The value must be a decimal number, followed by the suffix \u201c LUFS \u201d for Loudness Units Full Scale. For example, -9.317 LUFS . The value for album loudness must be consistent across all tracks in the album.","title":"Tags"},{"location":"loudness/#replaygain","text":"ReplayGain has been very influential for loudness normalization, and it is widely supported by both taggers and players, but unfortunately it has become ambiguous over time. ReplayGain does not store the loudness of the track directly, instead it stores the gain that is needed to bring the track to target loudness. The target loudness was initially well-defined, but tools started offering different options to accomodate to the reference loudness of different standards such as EBU R-128 , ATSC A/85 , and ReplayGain 2.0. This means that ReplayGain tags from different sources do not necessarily have the same meaning, which defeats the purpose of normalization. In practice this means that ReplayGain is only really useful if you can be certain that all tags were produced by the same program with the same settings. The BS17704_* tags used by Musium aim to sidestep this problem by storing the observed loudness, instead of the gain. The gain can easily be computed by the player, and the particular target loudness that is used is not important anyway for normalizing loudness in a collection of music. (It does matter when you want to match the loudness of your music to e.g. external streaming services.) Furthermore, by naming the tag after the revision of the standard, future revisions to BS.1770 will not create ambiguities in the meaning of existing tags.","title":"ReplayGain"},{"location":"loudness/#writing-bs1770-4-tags","text":"The BS1770 flacgain utility can be used to analyze a collection of flac files, and to add BS17704_* tags.","title":"Writing BS.1770-4 tags"},{"location":"performance/","text":"Performance Disk IO Should files be read from multiple threads, even when the disk is the bottleneck? By having multiple concurrent reads, the operating system might be able to optimize the disk access pattern, and schedule reads more efficiently for higher throughput. Let\u2019s measure. Disk Cache Threads Time (seconds) Cold 64 106.632476927 Cold 64 106.155341479 Cold 64 104.968957864 Warm 64 0.065452067 Warm 64 0.065966143 Warm 64 0.067338459 Cold 6 109.032390370 Cold 6 108.156613210 Cold 6 110.175107966 Warm 6 0.056552910 Warm 6 0.051793717 Warm 6 0.057326269 Warm 6 0.056153033 Cold 1 131.265989187 Cold 1 130.512200200 Cold 1 130.496186066 Warm 1 0.145899503 Warm 1 0.140550669 Warm 1 0.140376767 Warm 1 0.146533344 This is for roughly 11500 files. Program output was redirected to /dev/null. Single-threaded taken from commit 4a5982ceb94b6a3dc575abce3c47e148dd28aa9f . Multi-threaded taken from commit cc06a48af7c8ea8b8b647443db1a26f77374f9e4 . Conclusion: multithreaded ingestion is advantageous, both when indexing from the disk, as well as when indexing from memory. There must be a CPU-bound part as well then. (On my system, for my workload, that is.) The next question then, is how much threads to use. 64 threads probably already takes all of the parallel gains, and the returns diminish quickly. It could be worth optimizing the number of threads for running time with a warm disk cache, and that would likely also perform almost optimally for a cold cache. Some more results after reducing the thread queue size and doing non-blocking pushes, to keep the queue sizes more even: Disk Cache Threads Queue Size Time (seconds) Cold 128 16 105.591609927 Cold 512 0 97.509055644 Cold 512 0 96.345510293 Cold 128 1 94.403741744 Cold 128 0 85.897972147 Cold 64 0 82.595254011 Cold 64 0 83.793832797 Cold 48 0 80.877349368 Cold 32 0 80.913407455 Cold 24 0 82.893433723 Cold 16 0 83.807142608 Cold 16 0 83.967152892 Warm 128 16 0.075636796 Warm 128 1 0.072041480 Warm 128 0 0.075571860 And without queues or channels, protecting the directory iterator with a mutex instead: Disk Cache Threads Time (seconds) Cold 48 83.731602753 Cold 48 83.806947689 Cold 24 81.919455988 Cold 24 80.765494864 Cold 12 82.537088779 Cold 12 83.135829488 Warm 48 0.056744610 Warm 24 0.059594100 Warm 24 0.054264233 Warm 12 0.056491306 Warm 12 0.056685518 Precollect At commit c6c611be9179d939dc5646dc43ab8bdf5ddc2962 , with 24 threads. First collecting discovered paths into a vec, and constructing the index by iterating over the paths in the vec. Is this the right thing to do, or should we put the paths iterator in a mutex directly? Measurement setup: echo 3 | sudo tee /proc/sys/vm/drop_caches perf stat target/release/musium ~/music Note that the server was disabled to terminate the program after indexing. Also, these results are not comparable to the previous numbers, as the library has grown, and more data is processed. Furthermore, I did not redirect stdout to /dev/null in this case, but for a cold disk cache that does not make so much of a difference anyway. Precollect Time (seconds) Vec precollect 1 91.870704962 Vec precollect 1 90.106878818 Vec precollect 1 90.031705480 Vec precollect 2 86.926306901 Vec precollect 2 86.876997701 Vec precollect 2 89.131675265 Iter, double alloc 93.370680604 Iter, double alloc 93.180283609 Iter, double alloc 93.259494622 Iter, single alloc 94.026253229 Iter, single alloc 94.147137607 Iter, single alloc 94.352803977 Note that I did upgrade Walkdir when switching from vector precollect to the iterator-based version, so the comparison may be unfair. The data collected before the switch is labelled \u201cVec precollect 1\u201d, the version after upgrading to Walkdir 2.1.4 is labelled \u201cVec precollect 2\u201d. Furtherore, Walkdir 2.1.4 requires copying the path (labelled \u201cdouble alloc\u201d). I made a small change to the crate to be able to avoid the copy and extra allocation (labelled \u201csingle alloc\u201d). Counterintuitively, copying the path returned by the iterator is faster than not copying it. It might have something to do with ordering; spending more time in the iterator lock is actually a good thing? Or maybe I should collect more data, and this is just a statistical fluctuation. Just storing the paths is definitely faster if the copy is avoided: copy - c(0.023223363, 0.022365082, 0.022318216, 0.022584837, 0.020660742, 0.023839308, 0.022084252, 0.021812114, 0.022180668, 0.019982074, 0.020979151, 0.023186709, 0.024758619, 0.022889618, 0.024148854, 0.024708654) noncopy - c(0.022403112, 0.021863389, 0.019650964, 0.020984869, 0.021901483, 0.021376926, 0.021668108, 0.021504715, 0.023730031, 0.021861766, 0.021060567, 0.021986531, 0.022680138, 0.019719019, 0.020053399, 0.021137137) t.test(copy, noncopy) # Welch Two Sample t-test # # data: copy and noncopy # t = 2.6055, df = 28.297, p-value = 0.01447 # alternative hypothesis: true difference in means is not equal to 0 # 95 percent confidence interval: # 0.000242829 0.002024684 # sample estimates: # mean of x mean of y # 0.02260764 0.02147388 So it is preferable to read many paths at once before processing them, perhaps due to better branch prediction. The gains are so big that the extra allocations and reallocations for storing the pathbuf pointers in a vec are totally worth it. It might be even better then to alternate beween scanning paths and processing them, to reduce peak memory usage, but let\u2019s not worry about that at this point. Fadvise Command: echo 3 | sudo tee /proc/sys/vm/drop_caches perf stat target/release/musium cache /pool/music /pool/volatile/covers dummy Measurements were performed with disks spinning. If the disks needed to spin up first, I restarted the measurement as soon as the disk was spinning. Baseline, commit bcb01aac03b72c6250823d44d2b4dd71887e387c : Disk Cache Tracks Wall time (seconds) User time (seconds) Sys time (seconds) Cold 15931 142.662233261 3.283129000 8.579975000 Cold 15931 147.348811539 3.236058000 8.641414000 Cold 15931 145.916103563 3.376106000 8.547039000 Warm 15931 0.346267741 0.987189000 0.427480000 Warm 15931 0.369951824 0.886352000 0.523628000 Warm 15931 0.372806305 0.929290000 0.480558000 Open files first, read later, commit 0f2d00be7ef2009fe19af79ae02ac29d11c766cf : Disk Cache Tracks Wall time (seconds) User time (seconds) Sys time (seconds) Cold 15931 200.334320084 4.513103000 10.766578000 Warm 15931 0.835945466 2.131593000 2.420703000 Use \u201cfrontier\u201d read pattern, commit 64371ff0aa834add77185531bae7160cfd6134ad : Disk Cache Tracks Wall time (seconds) User time (seconds) Sys time (seconds) Cold 15931 148.444013742 4.524398000 10.423234000 Cold 15931 147.144940804 4.670934000 10.321421000 Warm 15931 1.134759625 2.831797000 4.271261000 Warm 15931 1.204304762 3.183732000 4.562911000 After changing the IO queue size (and also tuning internal queue a bit), this could be brought down to 93 seconds, which suggests the win is really more in IO patterns, and for the warm case, simpler is probably better. $ cat /sys/block/sd{b,c,d}/queue/nr_requests 4 4 4 $ echo 2048 | sudo tee /sys/block/sd{b,c,d}/queue/nr_requests 2048 Just regular blocking IO again, but tuning the disk queue size, commit a1ac4d2100f6d0bd61f0be99fc31b7da260be6af : Disk Cache Tracks nr_requests Threads Wall time (seconds) User time (seconds) Sys time (seconds) Cold 15931 2048 24 91.411324389 3.086841000 6.717432000 Cold 15931 2048 24 93.145262004 3.103839000 7.002719000 Cold 15931 2048 128 72.864833181 2.616870000 5.983099000 Cold 15931 2048 128 73.701789903 2.642680000 5.953727000 Cold 15931 2048 256 72.805136103 2.503932000 5.793247000 Cold 15931 2048 256 72.108359036 2.625222000 5.811989000 Warm 15931 2048 24 0.320772413 0.952222000 0.421805000 Warm 15931 2048 24 0.359001663 0.986530000 0.390287000 Warm 15931 2048 128 0.370368452 0.927427000 0.480482000 Warm 15931 2048 128 0.454763022 0.930003000 0.776343000 Warm 15931 2048 256 0.410941427 0.922298000 0.711005000 Warm 15931 2048 256 0.365117437 0.928634000 0.472226000","title":"Performance"},{"location":"performance/#performance","text":"","title":"Performance"},{"location":"performance/#disk-io","text":"Should files be read from multiple threads, even when the disk is the bottleneck? By having multiple concurrent reads, the operating system might be able to optimize the disk access pattern, and schedule reads more efficiently for higher throughput. Let\u2019s measure. Disk Cache Threads Time (seconds) Cold 64 106.632476927 Cold 64 106.155341479 Cold 64 104.968957864 Warm 64 0.065452067 Warm 64 0.065966143 Warm 64 0.067338459 Cold 6 109.032390370 Cold 6 108.156613210 Cold 6 110.175107966 Warm 6 0.056552910 Warm 6 0.051793717 Warm 6 0.057326269 Warm 6 0.056153033 Cold 1 131.265989187 Cold 1 130.512200200 Cold 1 130.496186066 Warm 1 0.145899503 Warm 1 0.140550669 Warm 1 0.140376767 Warm 1 0.146533344 This is for roughly 11500 files. Program output was redirected to /dev/null. Single-threaded taken from commit 4a5982ceb94b6a3dc575abce3c47e148dd28aa9f . Multi-threaded taken from commit cc06a48af7c8ea8b8b647443db1a26f77374f9e4 . Conclusion: multithreaded ingestion is advantageous, both when indexing from the disk, as well as when indexing from memory. There must be a CPU-bound part as well then. (On my system, for my workload, that is.) The next question then, is how much threads to use. 64 threads probably already takes all of the parallel gains, and the returns diminish quickly. It could be worth optimizing the number of threads for running time with a warm disk cache, and that would likely also perform almost optimally for a cold cache. Some more results after reducing the thread queue size and doing non-blocking pushes, to keep the queue sizes more even: Disk Cache Threads Queue Size Time (seconds) Cold 128 16 105.591609927 Cold 512 0 97.509055644 Cold 512 0 96.345510293 Cold 128 1 94.403741744 Cold 128 0 85.897972147 Cold 64 0 82.595254011 Cold 64 0 83.793832797 Cold 48 0 80.877349368 Cold 32 0 80.913407455 Cold 24 0 82.893433723 Cold 16 0 83.807142608 Cold 16 0 83.967152892 Warm 128 16 0.075636796 Warm 128 1 0.072041480 Warm 128 0 0.075571860 And without queues or channels, protecting the directory iterator with a mutex instead: Disk Cache Threads Time (seconds) Cold 48 83.731602753 Cold 48 83.806947689 Cold 24 81.919455988 Cold 24 80.765494864 Cold 12 82.537088779 Cold 12 83.135829488 Warm 48 0.056744610 Warm 24 0.059594100 Warm 24 0.054264233 Warm 12 0.056491306 Warm 12 0.056685518","title":"Disk IO"},{"location":"performance/#precollect","text":"At commit c6c611be9179d939dc5646dc43ab8bdf5ddc2962 , with 24 threads. First collecting discovered paths into a vec, and constructing the index by iterating over the paths in the vec. Is this the right thing to do, or should we put the paths iterator in a mutex directly? Measurement setup: echo 3 | sudo tee /proc/sys/vm/drop_caches perf stat target/release/musium ~/music Note that the server was disabled to terminate the program after indexing. Also, these results are not comparable to the previous numbers, as the library has grown, and more data is processed. Furthermore, I did not redirect stdout to /dev/null in this case, but for a cold disk cache that does not make so much of a difference anyway. Precollect Time (seconds) Vec precollect 1 91.870704962 Vec precollect 1 90.106878818 Vec precollect 1 90.031705480 Vec precollect 2 86.926306901 Vec precollect 2 86.876997701 Vec precollect 2 89.131675265 Iter, double alloc 93.370680604 Iter, double alloc 93.180283609 Iter, double alloc 93.259494622 Iter, single alloc 94.026253229 Iter, single alloc 94.147137607 Iter, single alloc 94.352803977 Note that I did upgrade Walkdir when switching from vector precollect to the iterator-based version, so the comparison may be unfair. The data collected before the switch is labelled \u201cVec precollect 1\u201d, the version after upgrading to Walkdir 2.1.4 is labelled \u201cVec precollect 2\u201d. Furtherore, Walkdir 2.1.4 requires copying the path (labelled \u201cdouble alloc\u201d). I made a small change to the crate to be able to avoid the copy and extra allocation (labelled \u201csingle alloc\u201d). Counterintuitively, copying the path returned by the iterator is faster than not copying it. It might have something to do with ordering; spending more time in the iterator lock is actually a good thing? Or maybe I should collect more data, and this is just a statistical fluctuation. Just storing the paths is definitely faster if the copy is avoided: copy - c(0.023223363, 0.022365082, 0.022318216, 0.022584837, 0.020660742, 0.023839308, 0.022084252, 0.021812114, 0.022180668, 0.019982074, 0.020979151, 0.023186709, 0.024758619, 0.022889618, 0.024148854, 0.024708654) noncopy - c(0.022403112, 0.021863389, 0.019650964, 0.020984869, 0.021901483, 0.021376926, 0.021668108, 0.021504715, 0.023730031, 0.021861766, 0.021060567, 0.021986531, 0.022680138, 0.019719019, 0.020053399, 0.021137137) t.test(copy, noncopy) # Welch Two Sample t-test # # data: copy and noncopy # t = 2.6055, df = 28.297, p-value = 0.01447 # alternative hypothesis: true difference in means is not equal to 0 # 95 percent confidence interval: # 0.000242829 0.002024684 # sample estimates: # mean of x mean of y # 0.02260764 0.02147388 So it is preferable to read many paths at once before processing them, perhaps due to better branch prediction. The gains are so big that the extra allocations and reallocations for storing the pathbuf pointers in a vec are totally worth it. It might be even better then to alternate beween scanning paths and processing them, to reduce peak memory usage, but let\u2019s not worry about that at this point.","title":"Precollect"},{"location":"performance/#fadvise","text":"Command: echo 3 | sudo tee /proc/sys/vm/drop_caches perf stat target/release/musium cache /pool/music /pool/volatile/covers dummy Measurements were performed with disks spinning. If the disks needed to spin up first, I restarted the measurement as soon as the disk was spinning. Baseline, commit bcb01aac03b72c6250823d44d2b4dd71887e387c : Disk Cache Tracks Wall time (seconds) User time (seconds) Sys time (seconds) Cold 15931 142.662233261 3.283129000 8.579975000 Cold 15931 147.348811539 3.236058000 8.641414000 Cold 15931 145.916103563 3.376106000 8.547039000 Warm 15931 0.346267741 0.987189000 0.427480000 Warm 15931 0.369951824 0.886352000 0.523628000 Warm 15931 0.372806305 0.929290000 0.480558000 Open files first, read later, commit 0f2d00be7ef2009fe19af79ae02ac29d11c766cf : Disk Cache Tracks Wall time (seconds) User time (seconds) Sys time (seconds) Cold 15931 200.334320084 4.513103000 10.766578000 Warm 15931 0.835945466 2.131593000 2.420703000 Use \u201cfrontier\u201d read pattern, commit 64371ff0aa834add77185531bae7160cfd6134ad : Disk Cache Tracks Wall time (seconds) User time (seconds) Sys time (seconds) Cold 15931 148.444013742 4.524398000 10.423234000 Cold 15931 147.144940804 4.670934000 10.321421000 Warm 15931 1.134759625 2.831797000 4.271261000 Warm 15931 1.204304762 3.183732000 4.562911000 After changing the IO queue size (and also tuning internal queue a bit), this could be brought down to 93 seconds, which suggests the win is really more in IO patterns, and for the warm case, simpler is probably better. $ cat /sys/block/sd{b,c,d}/queue/nr_requests 4 4 4 $ echo 2048 | sudo tee /sys/block/sd{b,c,d}/queue/nr_requests 2048 Just regular blocking IO again, but tuning the disk queue size, commit a1ac4d2100f6d0bd61f0be99fc31b7da260be6af : Disk Cache Tracks nr_requests Threads Wall time (seconds) User time (seconds) Sys time (seconds) Cold 15931 2048 24 91.411324389 3.086841000 6.717432000 Cold 15931 2048 24 93.145262004 3.103839000 7.002719000 Cold 15931 2048 128 72.864833181 2.616870000 5.983099000 Cold 15931 2048 128 73.701789903 2.642680000 5.953727000 Cold 15931 2048 256 72.805136103 2.503932000 5.793247000 Cold 15931 2048 256 72.108359036 2.625222000 5.811989000 Warm 15931 2048 24 0.320772413 0.952222000 0.421805000 Warm 15931 2048 24 0.359001663 0.986530000 0.390287000 Warm 15931 2048 128 0.370368452 0.927427000 0.480482000 Warm 15931 2048 128 0.454763022 0.930003000 0.776343000 Warm 15931 2048 256 0.410941427 0.922298000 0.711005000 Warm 15931 2048 256 0.365117437 0.928634000 0.472226000","title":"Fadvise"},{"location":"running/","text":"Running Musium logs to stdout and runs until it is killed, which makes it easy to run in a terminal for development, and it works well with systemd to run as a daemon. To run locally after building : target/release/musium serve musium.conf With systemd An example unit file: [Unit] Description=Musium Music Daemon [Service] # TODO: Currently the server loads static files from the repository, # so the working directory needs to be a checkout. We should embed the # static files in the binary instead. WorkingDirectory=/home/media/checkouts/musium ExecStart=/usr/local/bin/musium serve /etc/musium.conf [Install] WantedBy=default.target # TODO: Enable some hardening options. This assumes that you have a release binary in /usr/local/bin , and a configuration file at /etc/musium.conf . Write the above file to /etc/systemd/system/musium.service , then start the service: systemctl daemon-reload systemctl start musium With systemd-user It is also possible to run Musium using your systemd user instance. In that case, place the unit at ~/.config/systemd/user/musium.service , and use system --user to start it. If you run the deamon under your own account on a headless system, you may need to run loginctl enable-linger $USER to allow the deamon to linger after you log out.","title":"Running"},{"location":"running/#running","text":"Musium logs to stdout and runs until it is killed, which makes it easy to run in a terminal for development, and it works well with systemd to run as a daemon. To run locally after building : target/release/musium serve musium.conf","title":"Running"},{"location":"running/#with-systemd","text":"An example unit file: [Unit] Description=Musium Music Daemon [Service] # TODO: Currently the server loads static files from the repository, # so the working directory needs to be a checkout. We should embed the # static files in the binary instead. WorkingDirectory=/home/media/checkouts/musium ExecStart=/usr/local/bin/musium serve /etc/musium.conf [Install] WantedBy=default.target # TODO: Enable some hardening options. This assumes that you have a release binary in /usr/local/bin , and a configuration file at /etc/musium.conf . Write the above file to /etc/systemd/system/musium.service , then start the service: systemctl daemon-reload systemctl start musium","title":"With systemd"},{"location":"running/#with-systemd-user","text":"It is also possible to run Musium using your systemd user instance. In that case, place the unit at ~/.config/systemd/user/musium.service , and use system --user to start it. If you run the deamon under your own account on a headless system, you may need to run loginctl enable-linger $USER to allow the deamon to linger after you log out.","title":"With systemd-user"},{"location":"search/","text":"Search Each of the three Musium data types (artists, albums, and tracks) can be searched from a single search box, that searches all three simultaneously. To facilitate search, Musium maintains indexes from words to artists, albums, and tracks. Indexes are sorted on normalized word, so we can locate the words with a given prefix in logarithmic time. Single search Musium has a one search box. It should be able to find what you need from a single query, without the need to select what you are searching for, and without the need for separate browsers for artists, albums, and tracks, with independent search functions. Minimal results Search should find everything that is relevant, but nothing more. For example, when searching for \u201cqueen\u201d, we should show the artist Queen, but not every individual track by that artist, otherwise \u201cDancing Queen\u201d by Abba would get lost in that noise. Similarly, a search for an artist should not list all albums by that artist, unless they are relevant results by themselves. This happens for self-titled albums, but also for the word \u201cwho\u201d in \u201cWho\u2019s Next\u201d by The Who, for instance. Consider a search for \u201cabba dancing queen\u201d. Suppose we index the track artist in addition to the title, then we would find \u201cDancing Queen\u201d by Abba with this query. But we would also find it for the prefix \u201cabba\u201d, which is undesirable. Suppose we do not index the track artist, then we would not find the track at all, because the word \u201cabba\u201d does not occur in the title. On the other hand, if the track artist differs from the album artist (maybe because it includes a feat. artist, because the track is part of a compilation album), then we cannot reach that track through the artist search results, so then we do need to include the track itself, to make it discoverable. Similarly, a seac Conclusion: Words that occur in the track artist and also in the album artist, should not make the track show up, because we can also reach the track through the artist search result. If we consider the track for a different reason, then the presence of a word that does not occur in the track title, but which does occur in the track artist, should not cause the track to be excluded. Words that occur in the track artist, but not in the album artist, should make the track show up. Search combinations We want to support the following queries: Track title only, e.g. \u201cdancing queen\u201d Album title only, e.g. \u201carrival\u201d Album artist only, e.g. \u201cabba\u201d Track title and artist, e.g. \u201cabba dancing queen\u201d Album title and artist, e.g. \u201cabba arrival\u201d The following queries are out of scope: Track title and album, e.g. \u201carrival dancing queen\u201d Artist, album, and track, e.g. \u201cabba arrival dancing queen\u201d Indexes Based on the above considerations, we need the following indexes: Album artist words. Album title + album artist words, with a marker to tell whether the entry is for the album title or album artist. Track title + track artist words, with a marker to tell whether the entry is for the track title or artist, and if it is for the artist, a marker to tell whether the word occurs in the album artist too.","title":"Search"},{"location":"search/#search","text":"Each of the three Musium data types (artists, albums, and tracks) can be searched from a single search box, that searches all three simultaneously. To facilitate search, Musium maintains indexes from words to artists, albums, and tracks. Indexes are sorted on normalized word, so we can locate the words with a given prefix in logarithmic time.","title":"Search"},{"location":"search/#single-search","text":"Musium has a one search box. It should be able to find what you need from a single query, without the need to select what you are searching for, and without the need for separate browsers for artists, albums, and tracks, with independent search functions.","title":"Single search"},{"location":"search/#minimal-results","text":"Search should find everything that is relevant, but nothing more. For example, when searching for \u201cqueen\u201d, we should show the artist Queen, but not every individual track by that artist, otherwise \u201cDancing Queen\u201d by Abba would get lost in that noise. Similarly, a search for an artist should not list all albums by that artist, unless they are relevant results by themselves. This happens for self-titled albums, but also for the word \u201cwho\u201d in \u201cWho\u2019s Next\u201d by The Who, for instance. Consider a search for \u201cabba dancing queen\u201d. Suppose we index the track artist in addition to the title, then we would find \u201cDancing Queen\u201d by Abba with this query. But we would also find it for the prefix \u201cabba\u201d, which is undesirable. Suppose we do not index the track artist, then we would not find the track at all, because the word \u201cabba\u201d does not occur in the title. On the other hand, if the track artist differs from the album artist (maybe because it includes a feat. artist, because the track is part of a compilation album), then we cannot reach that track through the artist search results, so then we do need to include the track itself, to make it discoverable. Similarly, a seac Conclusion: Words that occur in the track artist and also in the album artist, should not make the track show up, because we can also reach the track through the artist search result. If we consider the track for a different reason, then the presence of a word that does not occur in the track title, but which does occur in the track artist, should not cause the track to be excluded. Words that occur in the track artist, but not in the album artist, should make the track show up.","title":"Minimal results"},{"location":"search/#search-combinations","text":"We want to support the following queries: Track title only, e.g. \u201cdancing queen\u201d Album title only, e.g. \u201carrival\u201d Album artist only, e.g. \u201cabba\u201d Track title and artist, e.g. \u201cabba dancing queen\u201d Album title and artist, e.g. \u201cabba arrival\u201d The following queries are out of scope: Track title and album, e.g. \u201carrival dancing queen\u201d Artist, album, and track, e.g. \u201cabba arrival dancing queen\u201d","title":"Search combinations"},{"location":"search/#indexes","text":"Based on the above considerations, we need the following indexes: Album artist words. Album title + album artist words, with a marker to tell whether the entry is for the album title or album artist. Track title + track artist words, with a marker to tell whether the entry is for the track title or artist, and if it is for the artist, a marker to tell whether the word occurs in the album artist too.","title":"Indexes"},{"location":"tagging/","text":"Tagging Musium reads metadata from flac tags (also called Vorbis comments) and maps those to its internal schema. Musium expects files to be tagged properly, and it expects the tags to be consistent across files. Because Musium uses internal ids based on MusicBrainz ids of albums and artists, MusicBrainz ids are required. Files tagged with MusicBrainz Picard should be fine. Schema Musium follows a tree-like data model. The library is a collection of artists. Every artist has one or more albums, and every album has one or more tracks. The track artist can differ from the album artist (for example, to accomodate feat. artists), but an album belongs to exactly one artist. For artists, albums, and tracks, Musium stores the following attributes: Artist Name Sort name Album Title Original release date Track Disc number Track number Title Artist Duration Tags Musium reads metadata from the following tags. Unless specified otherwise, all tags are mandatory. discnumber : Disc number, a non-negative integer less than 256. Defaults to 1 if not provided. tracknumber : Track number, a non-negative integer less than 256. title : Track title. artist : Track artist. album : Title of the album. albumartist : Name of the album artist. albumartistsort : Sort name of the album artist (e.g. name without articles). originaldate : Original release date of the album in YYYY-MM-DD format. If an exact date is not known, YYYY-MM and YYYY can be used instead. date : If originaldate is not provided, this field is used instead. musicbrainz_albumartistid : MusicBrainz id to group albums under. musicbrainz_albumid : MusicBrainz id to group tracks under. bs17704_track_loudness : Optional track loudness, see loudness normalization . bs17704_album_loudness : Optional album loudness, see loudness normalization . Note that duration is not read from the metadata. It is determined from the flac header instead. Consistency Tags contain redundant information, which must be consistent. For example, all tracks on the same album should have the same album artist and album title. Musium uses the MusicBrainz album id, to determine what album a track belongs to, and the MusicBrainz album artist id to determine which artist an album belongs to. If there is an inconsistency, Musium reports it, and it will then make an arbitrary choice about what version to keep.","title":"Tagging"},{"location":"tagging/#tagging","text":"Musium reads metadata from flac tags (also called Vorbis comments) and maps those to its internal schema. Musium expects files to be tagged properly, and it expects the tags to be consistent across files. Because Musium uses internal ids based on MusicBrainz ids of albums and artists, MusicBrainz ids are required. Files tagged with MusicBrainz Picard should be fine.","title":"Tagging"},{"location":"tagging/#schema","text":"Musium follows a tree-like data model. The library is a collection of artists. Every artist has one or more albums, and every album has one or more tracks. The track artist can differ from the album artist (for example, to accomodate feat. artists), but an album belongs to exactly one artist. For artists, albums, and tracks, Musium stores the following attributes:","title":"Schema"},{"location":"tagging/#artist","text":"Name Sort name","title":"Artist"},{"location":"tagging/#album","text":"Title Original release date","title":"Album"},{"location":"tagging/#track","text":"Disc number Track number Title Artist Duration","title":"Track"},{"location":"tagging/#tags","text":"Musium reads metadata from the following tags. Unless specified otherwise, all tags are mandatory. discnumber : Disc number, a non-negative integer less than 256. Defaults to 1 if not provided. tracknumber : Track number, a non-negative integer less than 256. title : Track title. artist : Track artist. album : Title of the album. albumartist : Name of the album artist. albumartistsort : Sort name of the album artist (e.g. name without articles). originaldate : Original release date of the album in YYYY-MM-DD format. If an exact date is not known, YYYY-MM and YYYY can be used instead. date : If originaldate is not provided, this field is used instead. musicbrainz_albumartistid : MusicBrainz id to group albums under. musicbrainz_albumid : MusicBrainz id to group tracks under. bs17704_track_loudness : Optional track loudness, see loudness normalization . bs17704_album_loudness : Optional album loudness, see loudness normalization . Note that duration is not read from the metadata. It is determined from the flac header instead.","title":"Tags"},{"location":"tagging/#consistency","text":"Tags contain redundant information, which must be consistent. For example, all tracks on the same album should have the same album artist and album title. Musium uses the MusicBrainz album id, to determine what album a track belongs to, and the MusicBrainz album artist id to determine which artist an album belongs to. If there is an inconsistency, Musium reports it, and it will then make an arbitrary choice about what version to keep.","title":"Consistency"},{"location":"thumbnails/","text":"Thumbnails Musium generates a cached thumbnail per album, extracted from the embedded cover art. The smaller thumbnails speed up the web-based library browser substantially. Initially, Musium used Imagemagick both for downsizing cover art, and for encoding the thumnails as jpeg. However, better compressors exist, and for some types of album art, in particular graphics with solid red areas, the thumbnails generated by Imagemagick showed pretty bad artefacts. This document compares compressors. Encoders considered The following encoders were compared: Imagemagick 7.0.9-10 Q16 Mozjpeg 3.3.1 Guetzli 1.0.1 These were used to encode 1216 downsized cover art images. They were downsized from their native size to 140 \u00d7 140 pixels with Imagemagick through convert infile -colorspace LAB -filter Cosine -distort Resize 140x140! -colorspace sRGB -strip outfile.png Size The cumulative size of all 1216 compressed thumbnails for various encoders: Size (bytes) Encoder Quality 10,771,743 Guetzli 95 11,926,551 Guetzli 96 12,016,780 Mozjpeg 92 12,889,658 Imagemagick 95 13,049,795 Mozjpeg 93 13,605,461 Guetzli 97 14,157,834 Mozjpeg 94 14,254,799 Imagemagick 96 14,988,104 Mozjpeg 95 19,531,105 Mozjpeg 97 20,254,930 Guetzli 99 The quality is the quality level passed to the encoder. The goal of this investigation was to find the most appropriate replacement for Imagemagick at quality 95 (which was used initially), but it is not obvious how to make a fair comparison when the sizes differ so greatly. Because the default of Imagemagick at quality 95 was a bit arbitrary, we can instead compare the 3 options that are closest together in terms of size, for a fairer comparison. Guetzli 96, Mozjpeg 92, Imagemagick 95, with a gap of 963,107 bytes. Imagemagick 95, Mozjpeg 93, Guetzli 97, with a gap of 715,803 bytes. Guetzli 97, Mozjpeg 94, Imagemagick 96, with a gap of 649,338 bytes. This means we will involve Imagemagick at quality 96 instead of 95, so the artefacts are not as bad, but they are still fairly apparent. Comparison Below is a comparison of the thumbnails where artefacts were most apparent in the images encoded by Imagemagick. This is not a general comparison of the encoders; the focus is specifically on images where Imagemagick performed badly. One thing that most of these images have in common, is that they contain solid areas of bright red with sharp edges. Guetzli 97 Mozjpeg 95 Imagemagick 96 A few things stand out visually: Both Guetzli and Mozjpeg perform much better than Imagemagick. Where Imagemagick creates blurry edges, Mozjpeg creates sharp edges, but at the cost of ringing artefacts. Guetzli produces sharp edges without much ringing. Throughput This comparison does not include timing information, but while running the various compressors, it was clear that Guetzli is much slower than either Imagemagick or Mozjpeg. For Musium this is not a big problem, because generating thumbnails tends to be IO -bound when the files are one a a spinning disk. As long as Guetzli is faster than the disk, it is fast enough. Conclusion We should compress thumbnails with Guetzli to minimize visible artefacts.","title":"Thumbnails"},{"location":"thumbnails/#thumbnails","text":"Musium generates a cached thumbnail per album, extracted from the embedded cover art. The smaller thumbnails speed up the web-based library browser substantially. Initially, Musium used Imagemagick both for downsizing cover art, and for encoding the thumnails as jpeg. However, better compressors exist, and for some types of album art, in particular graphics with solid red areas, the thumbnails generated by Imagemagick showed pretty bad artefacts. This document compares compressors.","title":"Thumbnails"},{"location":"thumbnails/#encoders-considered","text":"The following encoders were compared: Imagemagick 7.0.9-10 Q16 Mozjpeg 3.3.1 Guetzli 1.0.1 These were used to encode 1216 downsized cover art images. They were downsized from their native size to 140 \u00d7 140 pixels with Imagemagick through convert infile -colorspace LAB -filter Cosine -distort Resize 140x140! -colorspace sRGB -strip outfile.png","title":"Encoders considered"},{"location":"thumbnails/#size","text":"The cumulative size of all 1216 compressed thumbnails for various encoders: Size (bytes) Encoder Quality 10,771,743 Guetzli 95 11,926,551 Guetzli 96 12,016,780 Mozjpeg 92 12,889,658 Imagemagick 95 13,049,795 Mozjpeg 93 13,605,461 Guetzli 97 14,157,834 Mozjpeg 94 14,254,799 Imagemagick 96 14,988,104 Mozjpeg 95 19,531,105 Mozjpeg 97 20,254,930 Guetzli 99 The quality is the quality level passed to the encoder. The goal of this investigation was to find the most appropriate replacement for Imagemagick at quality 95 (which was used initially), but it is not obvious how to make a fair comparison when the sizes differ so greatly. Because the default of Imagemagick at quality 95 was a bit arbitrary, we can instead compare the 3 options that are closest together in terms of size, for a fairer comparison. Guetzli 96, Mozjpeg 92, Imagemagick 95, with a gap of 963,107 bytes. Imagemagick 95, Mozjpeg 93, Guetzli 97, with a gap of 715,803 bytes. Guetzli 97, Mozjpeg 94, Imagemagick 96, with a gap of 649,338 bytes. This means we will involve Imagemagick at quality 96 instead of 95, so the artefacts are not as bad, but they are still fairly apparent.","title":"Size"},{"location":"thumbnails/#comparison","text":"Below is a comparison of the thumbnails where artefacts were most apparent in the images encoded by Imagemagick. This is not a general comparison of the encoders; the focus is specifically on images where Imagemagick performed badly. One thing that most of these images have in common, is that they contain solid areas of bright red with sharp edges. Guetzli 97 Mozjpeg 95 Imagemagick 96 A few things stand out visually: Both Guetzli and Mozjpeg perform much better than Imagemagick. Where Imagemagick creates blurry edges, Mozjpeg creates sharp edges, but at the cost of ringing artefacts. Guetzli produces sharp edges without much ringing.","title":"Comparison"},{"location":"thumbnails/#throughput","text":"This comparison does not include timing information, but while running the various compressors, it was clear that Guetzli is much slower than either Imagemagick or Mozjpeg. For Musium this is not a big problem, because generating thumbnails tends to be IO -bound when the files are one a a spinning disk. As long as Guetzli is faster than the disk, it is fast enough.","title":"Throughput"},{"location":"thumbnails/#conclusion","text":"We should compress thumbnails with Guetzli to minimize visible artefacts.","title":"Conclusion"},{"location":"theme/README.theme/","text":"Kilsbergen A clean MkDocs theme. This theme is designed for Tako , Pris , and Noblit . It is not flexible on purpose: it supports everything I need, and nothing more. Demos Noblit documentation Pris documentation Tako documentation Features Responsive design Zero javascript License Kilsbergen is licensed under the Apache 2.0 license. In the generated documentation, it is fine to just link to this readme from a comment.","title":"Kilsbergen"},{"location":"theme/README.theme/#kilsbergen","text":"A clean MkDocs theme. This theme is designed for Tako , Pris , and Noblit . It is not flexible on purpose: it supports everything I need, and nothing more.","title":"Kilsbergen"},{"location":"theme/README.theme/#demos","text":"Noblit documentation Pris documentation Tako documentation","title":"Demos"},{"location":"theme/README.theme/#features","text":"Responsive design Zero javascript","title":"Features"},{"location":"theme/README.theme/#license","text":"Kilsbergen is licensed under the Apache 2.0 license. In the generated documentation, it is fine to just link to this readme from a comment.","title":"License"},{"location":"README.theme/","text":"Kilsbergen A clean MkDocs theme. This theme is designed for Tako , Pris , and Noblit . It is not flexible on purpose: it supports everything I need, and nothing more. Demos Noblit documentation Pris documentation Tako documentation Features Responsive design Zero javascript License Kilsbergen is licensed under the Apache 2.0 license. In the generated documentation, it is fine to just link to this readme from a comment.","title":"Kilsbergen"},{"location":"README.theme/#kilsbergen","text":"A clean MkDocs theme. This theme is designed for Tako , Pris , and Noblit . It is not flexible on purpose: it supports everything I need, and nothing more.","title":"Kilsbergen"},{"location":"README.theme/#demos","text":"Noblit documentation Pris documentation Tako documentation","title":"Demos"},{"location":"README.theme/#features","text":"Responsive design Zero javascript","title":"Features"},{"location":"README.theme/#license","text":"Kilsbergen is licensed under the Apache 2.0 license. In the generated documentation, it is fine to just link to this readme from a comment.","title":"License"}]}